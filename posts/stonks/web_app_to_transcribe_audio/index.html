<!DOCTYPE html><html lang="en"> <head><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Simple web app to transcribe audio - Dhanushka&#39;s Blog</title><meta name="description" content="Transcribing audio with python using openai and whispers"><meta name="author" content="Dhanushka Jayagoda"><link rel="alternate" type="application/rss+xml" href="/rss.xml"><link rel="icon" type="image/x-icon" href="/favicon.ico"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2479144310234386" crossorigin="anonymous"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-119155027-6"></script><script type="text/javascript">
      let slideIndex = 1;
      showSlides(slideIndex);

      function plusSlides(n) {
        showSlides((slideIndex += n));
      }

      function currentSlide(n) {
        showSlides((slideIndex = n));
      }

      function showSlides(n) {
        try {
          let i;
          let slides = document.getElementsByClassName('mySlides');
          let dots = document.getElementsByClassName('demo');
          let captionText = document.getElementById('caption');
          if (n > slides.length) {
            slideIndex = 1;
          }
          if (n < 1) {
            slideIndex = slides.length;
          }
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = 'none';
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(' active', '');
          }
          slides[slideIndex - 1].style.display = 'block';
          dots[slideIndex - 1].className += ' active';
          captionText.innerHTML = dots[slideIndex - 1].alt;
        } catch (err) {
          // do nothing here
        }
      }
    </script><!-- Google Tag Manager --><!-- End Google Tag Manager --><link rel="stylesheet" href="/_astro/index.BsaAxKtc.css" />
<style>.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}@media print{.no-print,.no-print *{display:none!important}}
</style><script type="module" src="/_astro/hoisted.UksxmqGZ.js"></script></head> <body class="bg-slate-900 text-gray-100"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K8P5P8FQ" height="0" width="0" style="display:none;visibility:hidden"></iframe> </noscript> <!-- End Google Tag Manager (noscript) --> <div class="mx-auto max-w-screen-lg px-3 py-6"><div class="flex flex-col gap-y-3 sm:flex-row sm:items-center sm:justify-between"><a href="/"><div class="flex items-center bg-gradient-to-br from-sky-500 to-cyan-400 bg-clip-text text-xl font-bold text-transparent"><svg class="mr-1 h-10 w-10 stroke-cyan-600" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"></path><rect x="3" y="12" width="6" height="8" rx="1"></rect><rect x="9" y="8" width="6" height="12" rx="1"></rect><rect x="15" y="4" width="6" height="16" rx="1"></rect><path d="M4 20h14"></path></svg>Dhanushka&#x27;s Blog</div></a><nav><ul class="flex gap-x-3 font-medium text-gray-200"><li class="hover:text-white"><a href="/posts">Blogs</a></li><li class="hover:text-white"><a href="https://github.com/dhanushka2001/blog-astro">GitHub</a></li><li class="hover:text-white"><a href="/photos">Photos</a></li></ul></nav></div></div> <div data-pagefind-body>  <div class="mx-auto max-w-screen-lg px-3 py-6"><div><h1 class="text-center text-3xl font-bold">Simple web app to transcribe audio</h1><div class="text-center text-sm text-gray-400"><div class="mt-1">Published:<!-- --> <!-- -->Mar 5, 2023</div><div class="mt-2 flex flex-wrap items-center justify-center gap-4"></div></div><div class="flex place-content-center pt-2"><div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/whisper" style="padding-right:3px"> <category>whisper<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/gradio" style="padding-right:3px"> <category>gradio<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-green-400 text-green-900"> <a href="/tags/python" style="padding-right:3px"> <category>python<!-- --> </category></a></div> </div><div class="mx-auto mt-5 max-w-prose"><div class="aspect-h-2 aspect-w-3"><img class="h-full w-full rounded-lg object-cover object-center" src="/imgs/2022/dall-e/DALL·E 2022-12-14 22.06.43 - blank book open on the coffee table.png" alt="rbc stock analyzer" loading="lazy"/></div><div class="prose prose-invert mt-8 prose-img:rounded-lg"><content> <h2 id="summary">Summary</h2>
<p>Audio transcription is the process of converting speech in an audio recording into written text. This can be useful in a variety of situations, such as when a person wants to search for a specific word or phrase within an audio recording, or when a person wants to make the content of an audio recording more accessible to people who are deaf or hard of hearing. Audio transcription can also be useful for creating written records of meetings, interviews, and other events that are recorded in audio form. In general, audio transcription can help to make the information in audio recordings more easily accessible and searchable.</p>
<p>Audio transcription can be useful for transcribing investor calls, which are typically audio recordings of meetings or conference calls where company executives discuss the company’s financial performance and outlook with investors. Transcribing these calls can make the information contained in them more easily accessible and searchable, which can be helpful for investors who want to review the information discussed in the calls at a later date.</p>
<p>The contents of requirements.txt are as follows:</p>
<pre class="astro-code monokai" style="background-color:#272822;color:#F8F8F2; overflow-x: auto;" tabindex="0"><code><span class="line"><span>whispers</span></span>
<span class="line"><span>git+https://github.com/openai/whisper.git</span></span></code></pre>
<pre class="astro-code monokai" style="background-color:#272822;color:#F8F8F2; overflow-x: auto;" tabindex="0"><code><span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> whisper</span></span>
<span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> gradio </span><span style="color:#F92672">as</span><span style="color:#F8F8F2"> gr</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic">def</span><span style="color:#A6E22E"> get_text_from_mp3_whisper</span><span style="color:#F8F8F2">(</span><span style="color:#FD971F;font-style:italic">mp3_file</span><span style="color:#F8F8F2">: </span><span style="color:#66D9EF;font-style:italic">str</span><span style="color:#F8F8F2">)-></span><span style="color:#66D9EF;font-style:italic">str</span><span style="color:#F8F8F2">:</span></span>
<span class="line"><span style="color:#F8F8F2">    model </span><span style="color:#F92672">=</span><span style="color:#F8F8F2"> whisper.load_model(</span><span style="color:#E6DB74">"base"</span><span style="color:#F8F8F2">)</span></span>
<span class="line"><span style="color:#88846F">    # options = whisper.DecodingOptions(language="en", without_timestamps=True)</span></span>
<span class="line"><span style="color:#F8F8F2">    result </span><span style="color:#F92672">=</span><span style="color:#F8F8F2"> model.transcribe(mp3_file)</span></span>
<span class="line"><span style="color:#F92672">    return</span><span style="color:#F8F8F2"> result.get(</span><span style="color:#E6DB74">"text"</span><span style="color:#F8F8F2">, </span><span style="color:#E6DB74">"No text found"</span><span style="color:#F8F8F2">), result.get(</span><span style="color:#E6DB74">"segments"</span><span style="color:#F8F8F2">, {})</span></span>
<span class="line"><span style="color:#F8F8F2">    </span></span>
<span class="line"><span style="color:#F8F8F2">    </span></span>
<span class="line"><span style="color:#F8F8F2"> </span></span>
<span class="line"><span style="color:#F8F8F2">gr.Interface(</span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">    title</span><span style="color:#F92672"> =</span><span style="color:#E6DB74"> 'OpenAI Whisper Transcribe audio files to text'</span><span style="color:#F8F8F2">, </span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">    fn</span><span style="color:#F92672">=</span><span style="color:#F8F8F2">get_text_from_mp3_whisper, </span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">    inputs</span><span style="color:#F92672">=</span><span style="color:#F8F8F2">[</span></span>
<span class="line"><span style="color:#F8F8F2">        gr.inputs.Audio(</span><span style="color:#FD971F;font-style:italic">type</span><span style="color:#F92672">=</span><span style="color:#E6DB74">"filepath"</span><span style="color:#F8F8F2">)</span></span>
<span class="line"><span style="color:#F8F8F2">    ],</span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">    outputs</span><span style="color:#F92672">=</span><span style="color:#F8F8F2">[</span></span>
<span class="line"><span style="color:#E6DB74">        "textbox"</span><span style="color:#F8F8F2">, </span><span style="color:#E6DB74">"json"</span></span>
<span class="line"><span style="color:#F8F8F2">    ],</span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">    live</span><span style="color:#F92672">=</span><span style="color:#AE81FF">True</span><span style="color:#F8F8F2">).launch()</span></span></code></pre>
<p>This code uses the whisper and gradio libraries to create an interface for transcribing audio files. The get_text_from_mp3_whisper function uses the whisper library to load a model and transcribe the audio file at the given filepath. The transcription results are returned as text and a JSON object. The gradio library is then used to create an interface that allows users to input an audio file and see the transcription results. The interface also has a live mode, which allows users to see the transcription results as they are being generated.</p>
<h2 id="references">References</h2>
<p>This app is hosted on hugging space</p>
<p>See <a href="https://huggingface.co/spaces/FriendlyUser/whisper_demo">https://huggingface.co/spaces/FriendlyUser/whisper_demo</a></p> </content></div></div></div></div>  </div> <div class="mx-auto max-w-screen-lg px-3 py-6"><div class="no-print border-t border-gray-600 pt-5"><div class="text-sm text-gray-200">© Copyright <!-- -->2025<!-- --> by <!-- -->Dhanushka&#x27;s Blog<!-- -->. Built with ♥ by<!-- --> <a class="text-cyan-400 hover:underline" href="https://github.com/dhanushka2001" target="_blank" rel="noopener noreferrer">Dhanushka</a>. Last updated on <!-- -->2025-07-07<!-- -->.</div></div><script src="https://cdn.botpress.cloud/webchat/v0/inject.js"></script><script src="https://mediafiles.botpress.cloud/0df54034-3318-451a-aed0-3923a4ee25a5/webchat/config.js" defer=""></script></div> </body></html>