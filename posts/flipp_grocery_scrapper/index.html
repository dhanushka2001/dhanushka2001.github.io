<!DOCTYPE html><html lang="en"> <head><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Automated Data Extraction from Online Retail Flyers Using Python and Selenium - Dhanushka&#39;s Blog</title><meta name="description" content="This article explores the `flipp_flyer_parser` Python script, an advanced web scraping tool for extracting promotional data from retail websites like Save-On-Foods, Walmart, and Superstore."><meta name="author" content="Dhanushka Jayagoda"><link rel="alternate" type="application/rss+xml" href="/rss.xml"><link rel="icon" type="image/x-icon" href="/favicon.ico"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2479144310234386" crossorigin="anonymous"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-119155027-6"></script><script type="text/javascript">
      let slideIndex = 1;
      showSlides(slideIndex);

      function plusSlides(n) {
        showSlides((slideIndex += n));
      }

      function currentSlide(n) {
        showSlides((slideIndex = n));
      }

      function showSlides(n) {
        try {
          let i;
          let slides = document.getElementsByClassName('mySlides');
          let dots = document.getElementsByClassName('demo');
          let captionText = document.getElementById('caption');
          if (n > slides.length) {
            slideIndex = 1;
          }
          if (n < 1) {
            slideIndex = slides.length;
          }
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = 'none';
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(' active', '');
          }
          slides[slideIndex - 1].style.display = 'block';
          dots[slideIndex - 1].className += ' active';
          captionText.innerHTML = dots[slideIndex - 1].alt;
        } catch (err) {
          // do nothing here
        }
      }
    </script><!-- Google Tag Manager --><!-- End Google Tag Manager --><link rel="stylesheet" href="/_astro/index.B_2FsOPP.css" />
<style>.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}@media print{.no-print,.no-print *{display:none!important}}
</style><script type="module" src="/_astro/hoisted.UksxmqGZ.js"></script></head> <body class="bg-slate-900 text-gray-100"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K8P5P8FQ" height="0" width="0" style="display:none;visibility:hidden"></iframe> </noscript> <!-- End Google Tag Manager (noscript) --> <div class="mx-auto max-w-screen-lg px-3 py-6"><div class="flex flex-col gap-y-3 sm:flex-row sm:items-center sm:justify-between"><a href="/"><div class="flex items-center bg-gradient-to-br from-sky-500 to-cyan-400 bg-clip-text text-xl font-bold text-transparent"><svg class="mr-1 h-10 w-10 stroke-cyan-600" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"></path><rect x="3" y="12" width="6" height="8" rx="1"></rect><rect x="9" y="8" width="6" height="12" rx="1"></rect><rect x="15" y="4" width="6" height="16" rx="1"></rect><path d="M4 20h14"></path></svg>Dhanushka&#x27;s Blog</div></a><nav><ul class="flex gap-x-3 font-medium text-gray-200"><li class="hover:text-white"><a href="/posts">Blogs</a></li><li class="hover:text-white"><a href="https://github.com/dhanushka2001/blog-astro">GitHub</a></li><li class="hover:text-white"><a href="/photos">Photos</a></li></ul></nav></div></div> <div data-pagefind-body>  <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();;(()=>{var v=Object.defineProperty;var A=(c,s,a)=>s in c?v(c,s,{enumerable:!0,configurable:!0,writable:!0,value:a}):c[s]=a;var d=(c,s,a)=>(A(c,typeof s!="symbol"?s+"":s,a),a);var u;{let c={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t)},s=t=>{let[e,n]=t;return e in c?c[e](n):void 0},a=t=>t.map(s),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([e,n])=>[e,s(n)]));customElements.get("astro-island")||customElements.define("astro-island",(u=class extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var f;if(!this.hydrator||!this.isConnected)return;let e=(f=this.parentElement)==null?void 0:f.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let n=this.querySelectorAll("astro-slot"),r={},l=this.querySelectorAll("template[data-astro-template]");for(let o of l){let i=o.closest(this.tagName);i!=null&&i.isSameNode(this)&&(r[o.getAttribute("data-astro-template")||"default"]=o.innerHTML,o.remove())}for(let o of n){let i=o.closest(this.tagName);i!=null&&i.isSameNode(this)&&(r[o.getAttribute("name")||"default"]=o.innerHTML)}let h;try{h=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(o){let i=this.getAttribute("component-url")||"<unknown>",b=this.getAttribute("component-export");throw b&&(i+=` (export ${b})`),console.error(`[hydrate] Error parsing props for component ${i}`,this.getAttribute("props"),o),o}let p;await this.hydrator(this)(this.Component,h,r,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),n.disconnect(),this.childrenConnectedCallback()},n=new MutationObserver(()=>{var r;((r=this.lastChild)==null?void 0:r.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});n.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),n=this.getAttribute("client");if(Astro[n]===void 0){window.addEventListener(`astro:${n}`,()=>this.start(),{once:!0});return}try{await Astro[n](async()=>{let r=this.getAttribute("renderer-url"),[l,{default:h}]=await Promise.all([import(this.getAttribute("component-url")),r?import(r):()=>()=>{}]),p=this.getAttribute("component-export")||"default";if(!p.includes("."))this.Component=l[p];else{this.Component=l;for(let y of p.split("."))this.Component=this.Component[y]}return this.hydrator=h,this.hydrate},e,this)}catch(r){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,r)}}attributeChangedCallback(){this.hydrate()}},d(u,"observedAttributes",["props"]),u))}})();</script><astro-island uid="Zuef65" prefix="r3" component-url="/_astro/BlogPost._FxswCuE.js" component-export="default" renderer-url="/_astro/client.D9Vng9vH.js" props="{&quot;frontmatter&quot;:[0,{&quot;title&quot;:[0,&quot;Automated Data Extraction from Online Retail Flyers Using Python and Selenium&quot;],&quot;description&quot;:[0,&quot;This article explores the `flipp_flyer_parser` Python script, an advanced web scraping tool for extracting promotional data from retail websites like Save-On-Foods, Walmart, and Superstore.&quot;],&quot;alt&quot;:[0,&quot;Web Scraping Retail Flyers with Python&quot;],&quot;pubDate&quot;:[0,&quot;Thursday, 15 Feburary 2023 13:00:00 GMT&quot;],&quot;tags&quot;:[1,[[0,&quot;python&quot;],[0,&quot;web scraping&quot;],[0,&quot;selenium&quot;],[0,&quot;data extraction&quot;],[0,&quot;retail&quot;],[0,&quot;automation&quot;]]],&quot;imgAlt&quot;:[0,&quot;Python Web Scraping Visualization&quot;],&quot;imgSrc&quot;:[0,&quot;/imgs/2023/117117315.png&quot;],&quot;authors&quot;:[1,[[0,&quot;David Li&quot;]]],&quot;file&quot;:[0,&quot;/home/runner/work/blog-astro/blog-astro/src/pages/posts/flipp_grocery_scrapper.md&quot;],&quot;url&quot;:[0,&quot;/posts/flipp_grocery_scrapper&quot;]}]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;BlogPost&quot;,&quot;value&quot;:true}" await-children=""><div class="mx-auto max-w-screen-lg px-3 py-6"><div><h1 class="text-center text-3xl font-bold">Automated Data Extraction from Online Retail Flyers Using Python and Selenium</h1><div class="text-center text-sm text-gray-400"><div class="mt-1">Published:<!-- --> <!-- -->Feb 15, 2023</div><div class="mt-2 flex flex-wrap items-center justify-center gap-4"><div class="flex items-center space-x-2"><img src="/assets/images/avatars/david.png" alt="David Li" class="h-6 w-6 rounded-full object-cover" loading="lazy"/><a href="https://github.com/FriendlyUser" target="_blank" rel="noopener noreferrer" class="hover:underline">David Li</a></div></div></div><div class="flex place-content-center pt-2"><div class="rounded-md px-2 py-1 text-xs font-semibold bg-green-400 text-green-900"> <a href="/tags/python" style="padding-right:3px"> <category>python<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/web scraping" style="padding-right:3px"> <category>web scraping<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/selenium" style="padding-right:3px"> <category>selenium<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/data extraction" style="padding-right:3px"> <category>data extraction<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/retail" style="padding-right:3px"> <category>retail<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/automation" style="padding-right:3px"> <category>automation<!-- --> </category></a></div> </div><div class="mx-auto mt-5 max-w-prose"><div class="aspect-h-2 aspect-w-3"><img class="h-full w-full rounded-lg object-cover object-center" src="/imgs/2023/117117315.png" alt="Python Web Scraping Visualization" loading="lazy"/></div><div class="prose prose-invert mt-6 prose-img:rounded-lg"><content><astro-slot> <h1 id="web-scraping-retail-flipp-flyers">Web Scraping Retail Flipp Flyers</h1>
<p>The <code>flipp_flyer_parser</code> Python script is a sophisticated web scraping tool, designed for extracting promotional flyer data from various retail websites. Authored by FriendlyUser, this script leverages Selenium, a powerful tool for browser automation, to navigate through web pages and extract relevant data. It focuses on three major Canadian retailers: Save-On-Foods, Walmart, and Superstore.</p>
<h2 id="key-components-and-libraries">Key Components and Libraries</h2>
<ul>
<li><strong>Selenium WebDriver (<code>undetected_chromedriver</code>)</strong>: Used for controlling a Chrome browser. This driver is essential for navigating through the web pages and interacting with web elements.</li>
<li><strong>Date Parsing (<code>dateutil.parser</code>)</strong>: Utilized for parsing date strings.</li>
<li><strong>Regular Expressions (<code>re</code>)</strong>: Employed for text pattern matching and data extraction from descriptions.</li>
<li><strong>Image Processing (<code>PIL</code>)</strong>: The Python Imaging Library (PIL) can be used for handling images, though its specific usage isn’t clear from the provided script.</li>
<li><strong>Argument Parsing (<code>argparse</code>)</strong>: Facilitates command-line argument parsing, allowing users to specify the store type.</li>
</ul>
<h2 id="core-functionalities">Core Functionalities</h2>
<h3 id="webdriver-setup">WebDriver Setup</h3>
<ul>
<li><code>make_driver()</code>: Creates a Chrome WebDriver instance with optional headless browsing.</li>
<li>Specific setup functions for each store (<code>selenium_setup_saveon</code>, <code>selenium_setup_walmart</code>, <code>setup_superstore</code>): These functions initialize the WebDriver and navigate to the respective store’s flyer page.</li>
</ul>
<h3 id="data-extraction">Data Extraction</h3>
<ul>
<li><code>parse_flipp_aside(driver, cfg)</code>: Extracts detailed information from a specific part of the webpage (flipp aside iframe). It retrieves various data like start and end dates, product descriptions, sizes, quantities, and more.</li>
<li><code>scrap_flyer(driver, cfg)</code>: Orchestrates the scraping process. It involves navigating through iframes, handling cookies, extracting HTML content, and iterating over flyer images to gather product data.</li>
</ul>
<h3 id="utility-functions">Utility Functions</h3>
<ul>
<li><code>swap_to_iframe(driver, iframe_class)</code>: Aids in switching between different iframes within a webpage.</li>
<li>StoreType <code>Enum</code>: Enumerates store types (SAVEON, WALMART, SUPERSTORE) for easier management.</li>
</ul>
<h3 id="main-function">Main Function</h3>
<ul>
<li>The script uses a command-line interface where the user can specify the store type.</li>
<li>Based on the store type, it calls the corresponding function to set up Selenium and scrape the flyer data.</li>
</ul>
<h2 id="technical-highlights">Technical Highlights</h2>
<h3 id="use-of-selenium">Use of Selenium</h3>
<ul>
<li>The script showcases an advanced use of Selenium for web scraping, handling dynamic content loaded through JavaScript and embedded iframes, which are common challenges in modern web scraping tasks.</li>
</ul>
<h3 id="regular-expressions">Regular Expressions</h3>
<ul>
<li>Extensive use of regex for parsing complex text patterns in product descriptions, which is crucial for accurate data extraction in web scraping projects.</li>
</ul>
<h3 id="error-handling">Error Handling</h3>
<ul>
<li>The script includes basic error handling, particularly in <code>parse_flipp_aside</code> and <code>scrap_flyer</code> functions, to manage exceptions like <code>NoSuchElementException</code>.</li>
</ul>
<h2 id="expanded-technical-analysis-of-flipp_flyer_parser">Expanded Technical Analysis of <code>flipp_flyer_parser</code></h2>
<p>We will delve into the more complex parts of the <code>flipp_flyer_parser</code> script, breaking down key functions and processes step by step.</p>
<h3 id="1-webdriver-setup">1. WebDriver Setup</h3>
<p>The script begins with setting up the Selenium WebDriver, crucial for browser automation.</p>
<h4 id="make_driver-function"><code>make_driver</code> Function</h4>
<ul>
<li>Initializes a Chrome WebDriver using <code>undetected_chromedriver</code>.</li>
<li>The function returns a WebDriver object with <code>headless=False</code> (meaning the browser UI is visible during scraping) and <code>use_subprocess=False</code>.</li>
</ul>
<h3 id="2-store-specific-setup-functions">2. Store-Specific Setup Functions</h3>
<p>These functions are tailored for each retail website, navigating to the respective flyer pages.</p>
<h4 id="selenium_setup_walmart-selenium_setup_saveon-and-setup_superstore-functions"><code>selenium_setup_walmart</code>, <code>selenium_setup_saveon</code>, and <code>setup_superstore</code> Functions</h4>
<ul>
<li>Each function creates a WebDriver instance via <code>make_driver()</code>.</li>
<li>Navigates to the specific URL of the store’s flyer page.</li>
<li>For <code>setup_superstore</code>, additional cookie manipulation is performed to mimic a user’s browser settings.</li>
</ul>
<h3 id="3-data-extraction">3. Data Extraction</h3>
<h4 id="parse_flipp_aside-function"><code>parse_flipp_aside</code> Function</h4>
<p>This function extracts detailed information from a part of the webpage, typically an iframe.</p>
<p><strong>Switching to the Relevant Iframe</strong>:</p>
<p>Calls <code>swap_to_iframe</code> with the class name of the iframe to be accessed.</p>
<p><strong>Extracting Information</strong>:</p>
<p>Finds elements by tag or class name (e.g., validity dates, descriptions).
Regular expressions are used to parse and extract data like sizes, quantities, and product types from the product description.
Exception handling is used to manage elements that might not be present.</p>
<h4 id="scrap_flyer-function"><code>scrap_flyer</code> Function</h4>
<p>This function orchestrates the overall scraping process.</p>
<p><strong>Initial Setup</strong>:</p>
<p>Waits for the main element of the page to become visible.
Handles exceptions by saving the page source to a file for debugging.</p>
<p><strong>Handling Cookies and HTML Content</strong>:
Retrieves and saves cookies to a JSON file.
Saves the HTML content of the page for further processing.</p>
<p><strong>Navigating Through Flyer Images</strong>:
Iterates over elements containing flyer images.
For each image, iterates over associated buttons that likely contain product information.
Executes a script to ensure the visibility of elements and interacts with them (clicking buttons).</p>
<p><strong>Extracting Product Data</strong>:
Each button’s label is parsed for product data.
Regular expressions are used to extract pricing information.
Calls <code>parse_flipp_aside</code> to extract additional details from the aside section.
Aggregates all extracted data into a dictionary and appends it to a list.</p>
<p><strong>Final Steps</strong>:
The data list is saved to a JSON file.
Handles a set maximum number of items to prevent excessive scraping.</p>
<h3 id="4-main-function">4. Main Function</h3>
<p>The script uses an argument parser to allow the user to specify the store type via the command line.</p>
<p>Based on the store type provided, the corresponding scraping function is called.
This modular approach allows for easy expansion or modification for different stores.</p>
<h3 id="conclusion">Conclusion</h3>
<p>The <code>flipp_flyer_parser</code> script is a comprehensive example of advanced web scraping using Python and Selenium. It demonstrates handling dynamic web content, navigating through complex webpage structures, and extracting structured data from unstructured HTML. The use of regular expressions and strategic error handling are notable for their efficiency in data parsing and resilience against web scraping challenges. This script serves as an excellent template for similar web scraping tasks, particularly those involving dynamic and interactive web content.</p> </astro-slot></content></div><div id="comments"></div></div></div></div><!--astro:end--></astro-island> <div class="mx-auto mt-5 max-w-prose flex justify-between gap-4 border-t border-gray-700 pt-8 text-sm text-gray-400 dark:text-gray-500"> <div class="w-full sm:w-[48%] xl:w-auto"> <div class="text-xs uppercase">Previous</div> <a class="text-blue-400 hover:underline" href="/posts/tech/js/introduction_to_docker_with_express"> Introduction to docker with express </a> </div> <div class="text-right w-full sm:w-[48%] xl:w-auto"> <div class="text-xs uppercase">Next</div> <a class="text-blue-400 hover:underline" href="/posts/tech/rust/how_to_handle_times_with_chrono_in_rust"> Using the Chrono Crate in Rust for Time and Date Handling </a> </div> </div><div class="mt-6 text-center"> <a href="/posts" class="text-blue-400 hover:underline text-sm">
&larr; Back to blog
</a> </div> <astro-island uid="1KiplR" prefix="r0" component-url="/_astro/ScrollButtons.BdAHGQrx.js" component-export="default" renderer-url="/_astro/client.D9Vng9vH.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;ScrollButtons&quot;,&quot;value&quot;:true}" await-children=""><div class="fixed bottom-8 right-8 z-50 flex flex-col gap-3 opacity-0 transition-opacity duration-300"><button aria-label="Scroll to comments" class="rounded-full bg-gray-200 p-2 text-gray-600 shadow-md hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-300 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll to top" class="rounded-full bg-gray-200 p-2 text-gray-600 shadow-md hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-300 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><!--astro:end--></astro-island>  </div> <div class="mx-auto max-w-screen-lg px-3 py-6"><div class="no-print border-t border-gray-600 pt-5"><div class="text-sm text-gray-200">© Copyright <!-- -->2025<!-- --> by <!-- -->Dhanushka&#x27;s Blog<!-- -->. Built with ♥ by<!-- --> <a class="text-cyan-400 hover:underline" href="https://github.com/dhanushka2001" target="_blank" rel="noopener noreferrer">Dhanushka</a>. Last updated on <!-- -->2025-07-11<!-- -->.</div></div></div> </body></html>