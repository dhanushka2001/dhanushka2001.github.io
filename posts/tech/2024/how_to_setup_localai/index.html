<!DOCTYPE html><html lang="en"> <head><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>The Importance of Large Language Models and Privacy with Local AI (On-Device) - Dhanushka&#39;s Blog</title><meta name="description" content="Explore the benefits of on-device AI for privacy-first applications using the nlux library and Google's built-in AI features. Learn how to leverage local AI capabilities for enhanced privacy, performance, and offline functionality."><meta name="author" content="Dhanushka Jayagoda"><link rel="alternate" type="application/rss+xml" href="/rss.xml"><link rel="icon" type="image/x-icon" href="/favicon.ico"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2479144310234386" crossorigin="anonymous"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-119155027-6"></script><script type="text/javascript">
      let slideIndex = 1;
      showSlides(slideIndex);

      function plusSlides(n) {
        showSlides((slideIndex += n));
      }

      function currentSlide(n) {
        showSlides((slideIndex = n));
      }

      function showSlides(n) {
        try {
          let i;
          let slides = document.getElementsByClassName('mySlides');
          let dots = document.getElementsByClassName('demo');
          let captionText = document.getElementById('caption');
          if (n > slides.length) {
            slideIndex = 1;
          }
          if (n < 1) {
            slideIndex = slides.length;
          }
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = 'none';
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(' active', '');
          }
          slides[slideIndex - 1].style.display = 'block';
          dots[slideIndex - 1].className += ' active';
          captionText.innerHTML = dots[slideIndex - 1].alt;
        } catch (err) {
          // do nothing here
        }
      }
    </script><!-- Google Tag Manager --><!-- End Google Tag Manager --><link rel="stylesheet" href="/_astro/index.BsaAxKtc.css" />
<style>.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}@media print{.no-print,.no-print *{display:none!important}}
</style><script type="module" src="/_astro/hoisted.UksxmqGZ.js"></script></head> <body class="bg-slate-900 text-gray-100"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K8P5P8FQ" height="0" width="0" style="display:none;visibility:hidden"></iframe> </noscript> <!-- End Google Tag Manager (noscript) --> <div class="mx-auto max-w-screen-lg px-3 py-6"><div class="flex flex-col gap-y-3 sm:flex-row sm:items-center sm:justify-between"><a href="/"><div class="flex items-center bg-gradient-to-br from-sky-500 to-cyan-400 bg-clip-text text-xl font-bold text-transparent"><svg class="mr-1 h-10 w-10 stroke-cyan-600" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"></path><rect x="3" y="12" width="6" height="8" rx="1"></rect><rect x="9" y="8" width="6" height="12" rx="1"></rect><rect x="15" y="4" width="6" height="16" rx="1"></rect><path d="M4 20h14"></path></svg>Dhanushka&#x27;s Blog</div></a><nav><ul class="flex gap-x-3 font-medium text-gray-200"><li class="hover:text-white"><a href="/posts">Blogs</a></li><li class="hover:text-white"><a href="https://github.com/dhanushka2001/blog-astro">GitHub</a></li><li class="hover:text-white"><a href="/photos">Photos</a></li></ul></nav></div></div> <div data-pagefind-body>  <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();;(()=>{var v=Object.defineProperty;var A=(c,s,a)=>s in c?v(c,s,{enumerable:!0,configurable:!0,writable:!0,value:a}):c[s]=a;var d=(c,s,a)=>(A(c,typeof s!="symbol"?s+"":s,a),a);var u;{let c={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t)},s=t=>{let[e,n]=t;return e in c?c[e](n):void 0},a=t=>t.map(s),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([e,n])=>[e,s(n)]));customElements.get("astro-island")||customElements.define("astro-island",(u=class extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var f;if(!this.hydrator||!this.isConnected)return;let e=(f=this.parentElement)==null?void 0:f.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let n=this.querySelectorAll("astro-slot"),r={},l=this.querySelectorAll("template[data-astro-template]");for(let o of l){let i=o.closest(this.tagName);i!=null&&i.isSameNode(this)&&(r[o.getAttribute("data-astro-template")||"default"]=o.innerHTML,o.remove())}for(let o of n){let i=o.closest(this.tagName);i!=null&&i.isSameNode(this)&&(r[o.getAttribute("name")||"default"]=o.innerHTML)}let h;try{h=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(o){let i=this.getAttribute("component-url")||"<unknown>",b=this.getAttribute("component-export");throw b&&(i+=` (export ${b})`),console.error(`[hydrate] Error parsing props for component ${i}`,this.getAttribute("props"),o),o}let p;await this.hydrator(this)(this.Component,h,r,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),n.disconnect(),this.childrenConnectedCallback()},n=new MutationObserver(()=>{var r;((r=this.lastChild)==null?void 0:r.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});n.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),n=this.getAttribute("client");if(Astro[n]===void 0){window.addEventListener(`astro:${n}`,()=>this.start(),{once:!0});return}try{await Astro[n](async()=>{let r=this.getAttribute("renderer-url"),[l,{default:h}]=await Promise.all([import(this.getAttribute("component-url")),r?import(r):()=>()=>{}]),p=this.getAttribute("component-export")||"default";if(!p.includes("."))this.Component=l[p];else{this.Component=l;for(let y of p.split("."))this.Component=this.Component[y]}return this.hydrator=h,this.hydrate},e,this)}catch(r){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,r)}}attributeChangedCallback(){this.hydrate()}},d(u,"observedAttributes",["props"]),u))}})();</script><astro-island uid="ZbIi5t" prefix="r2" component-url="/_astro/BlogPost.BdPqyukv.js" component-export="default" renderer-url="/_astro/client.D9Vng9vH.js" props="{&quot;frontmatter&quot;:[0,{&quot;title&quot;:[0,&quot;The Importance of Large Language Models and Privacy with Local AI (On-Device)&quot;],&quot;pubDate&quot;:[0,&quot;2024-07-06T12:00:00.000Z&quot;],&quot;description&quot;:[0,&quot;Explore the benefits of on-device AI for privacy-first applications using the nlux library and Google&#39;s built-in AI features. Learn how to leverage local AI capabilities for enhanced privacy, performance, and offline functionality.&quot;],&quot;tags&quot;:[1,[[0,&quot;AI&quot;],[0,&quot;On-Device AI&quot;],[0,&quot;Large Language Models&quot;],[0,&quot;Privacy&quot;],[0,&quot;nlux&quot;],[0,&quot;Google Gemini Nano&quot;],[0,&quot;Web Development&quot;],[0,&quot;Machine Learning&quot;],[0,&quot;Data Security&quot;],[0,&quot;Local AI&quot;]]],&quot;imgSrc&quot;:[0,&quot;https://images.unsplash.com/photo-1525338078858-d762b5e32f2c&quot;],&quot;authors&quot;:[1,[[0,&quot;David Li&quot;]]],&quot;file&quot;:[0,&quot;/home/runner/work/blog-astro/blog-astro/src/pages/posts/tech/2024/how_to_setup_localai.md&quot;],&quot;url&quot;:[0,&quot;/posts/tech/2024/how_to_setup_localai&quot;]}]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;BlogPost&quot;,&quot;value&quot;:true}" await-children=""><div class="mx-auto max-w-screen-lg px-3 py-6"><div><h1 class="text-center text-3xl font-bold">The Importance of Large Language Models and Privacy with Local AI (On-Device)</h1><div class="text-center text-sm text-gray-400"><div class="mt-1">Published:<!-- --> <!-- -->Jul 6, 2024</div><div class="mt-2 flex flex-wrap items-center justify-center gap-4"><div class="flex items-center space-x-2"><img src="/assets/images/avatars/david.png" alt="David Li" class="h-6 w-6 rounded-full object-cover" loading="lazy"/><a href="https://github.com/FriendlyUser" target="_blank" rel="noopener noreferrer" class="hover:underline">David Li</a></div></div></div><div class="flex place-content-center pt-2"><div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/AI" style="padding-right:3px"> <category>AI<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/On-Device AI" style="padding-right:3px"> <category>On-Device AI<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Large Language Models" style="padding-right:3px"> <category>Large Language Models<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Privacy" style="padding-right:3px"> <category>Privacy<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/nlux" style="padding-right:3px"> <category>nlux<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Google Gemini Nano" style="padding-right:3px"> <category>Google Gemini Nano<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Web Development" style="padding-right:3px"> <category>Web Development<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Machine Learning" style="padding-right:3px"> <category>Machine Learning<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Data Security" style="padding-right:3px"> <category>Data Security<!-- --> </category></a></div> <div class="rounded-md px-2 py-1 text-xs font-semibold bg-cyan-400 text-cyan-900"> <a href="/tags/Local AI" style="padding-right:3px"> <category>Local AI<!-- --> </category></a></div> </div><div class="mx-auto mt-5 max-w-prose"><div class="aspect-h-2 aspect-w-3"><img class="h-full w-full rounded-lg object-cover object-center" src="https://images.unsplash.com/photo-1525338078858-d762b5e32f2c" loading="lazy"/></div><div class="prose prose-invert mt-6 prose-img:rounded-lg"><content><astro-slot> <h2 id="the-importance-of-large-language-models-and-privacy-with-local-ai-on-device">The Importance of Large Language Models and Privacy with Local AI (On-Device)</h2>
<h2 id="introduction">Introduction</h2>
<p>Large Language Models (LLMs) have revolutionized the field of artificial intelligence by enabling machines to understand and generate human-like text. These models, like OpenAI’s GPT-4, offer a wide array of applications, from content generation to complex problem-solving. However, the deployment of these models often raises concerns about privacy and data security, especially when they rely on server-side processing. This article explores the importance of LLMs, the benefits of on-device AI, and provides a practical guide to building privacy-first AI applications using the nlux library and Google’s built-in AI features.</p>
<h3 id="the-role-of-large-language-models">The Role of Large Language Models</h3>
<p>LLMs are at the forefront of AI advancements, enabling applications across various domains such as healthcare, finance, education, and customer service. Their ability to understand context, generate coherent responses, and perform intricate tasks makes them invaluable. However, deploying these models typically requires significant computational resources, often hosted on powerful servers. This server-side approach, while effective, comes with several drawbacks:</p>
<ul>
<li><strong>Privacy Concerns</strong>: Sensitive data transmitted to servers can be vulnerable to breaches.</li>
<li><strong>Latency</strong>: Round-trip times to and from the server can introduce delays.</li>
<li><strong>Internet Dependency</strong>: Server-side models require a reliable internet connection.</li>
</ul>
<h3 id="advantages-of-on-device-ai">Advantages of On-Device AI</h3>
<p>On-device AI, or local AI, mitigates these issues by processing data directly on the user’s device. This approach offers several key benefits:</p>
<ul>
<li><strong>Enhanced Privacy</strong>: Data remains on the device, reducing the risk of breaches.</li>
<li><strong>Improved Performance</strong>: Local processing can reduce latency and provide faster responses.</li>
<li><strong>Offline Capabilities</strong>: AI functionalities remain available even without internet access.</li>
</ul>
<h3 id="getting-started-with-nlux">Getting Started with nlux</h3>
<p>The nlux library simplifies the integration of on-device AI into web applications. It provides tools to leverage local AI capabilities, ensuring data privacy and efficient performance. Here’s a step-by-step guide to get started with nlux and implement AI-powered features in your application.</p>
<h4 id="installation">Installation</h4>
<p>First, install the nlux library and its dependencies:</p>
<pre class="astro-code monokai" style="background-color:#272822;color:#F8F8F2; overflow-x: auto;" tabindex="0"><code><span class="line"><span style="color:#A6E22E">npm</span><span style="color:#E6DB74"> install</span><span style="color:#E6DB74"> @nlux/react</span></span>
<span class="line"><span style="color:#A6E22E">npm</span><span style="color:#E6DB74"> install</span><span style="color:#E6DB74"> @nlux/themes</span></span></code></pre>
<h4 id="setting-up-the-chat-component">Setting Up the Chat Component</h4>
<p>Create a new React component to integrate the AI chat functionality using nlux.</p>
<pre class="astro-code monokai" style="background-color:#272822;color:#F8F8F2; overflow-x: auto;" tabindex="0"><code><span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> { useMemo } </span><span style="color:#F92672">from</span><span style="color:#E6DB74"> 'react'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> { AiChat } </span><span style="color:#F92672">from</span><span style="color:#E6DB74"> '@nlux/react'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"><span style="color:#F92672">import</span><span style="color:#E6DB74"> '@nlux/themes/nova.css'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> { streamAdapter } </span><span style="color:#F92672">from</span><span style="color:#E6DB74"> '../../utils/adapter'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> { personas } </span><span style="color:#F92672">from</span><span style="color:#E6DB74"> '../persona'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> withAI </span><span style="color:#F92672">from</span><span style="color:#E6DB74"> './withAi'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#66D9EF;font-style:italic">function</span><span style="color:#A6E22E"> Chat</span><span style="color:#F8F8F2">() {</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic">  const</span><span style="color:#F8F8F2"> adapter </span><span style="color:#F92672">=</span><span style="color:#A6E22E"> useMemo</span><span style="color:#F8F8F2">(() </span><span style="color:#66D9EF;font-style:italic">=></span><span style="color:#F8F8F2"> streamAdapter, []);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672">  return</span><span style="color:#F8F8F2"> (</span></span>
<span class="line"><span style="color:#F8F8F2">    &#x3C;</span><span style="color:#66D9EF;font-style:italic">AiChat</span></span>
<span class="line"><span style="color:#A6E22E">      adapter</span><span style="color:#F92672">={</span><span style="color:#F8F8F2">adapter</span><span style="color:#F92672">}</span></span>
<span class="line"><span style="color:#A6E22E">      personaOptions</span><span style="color:#F92672">={</span><span style="color:#F8F8F2">personas</span><span style="color:#F92672">}</span></span>
<span class="line"><span style="color:#A6E22E">      displayOptions</span><span style="color:#F92672">={</span><span style="color:#F8F8F2">{ colorScheme: </span><span style="color:#E6DB74">'dark'</span><span style="color:#F8F8F2"> }</span><span style="color:#F92672">}</span></span>
<span class="line"><span style="color:#F8F8F2">    /></span></span>
<span class="line"><span style="color:#F8F8F2">  );</span></span>
<span class="line"><span style="color:#F8F8F2">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672">export</span><span style="color:#F92672"> default</span><span style="color:#A6E22E"> withAI</span><span style="color:#F8F8F2">(Chat);</span></span></code></pre>
<h4 id="streaming-ai-responses-with-windowai">Streaming AI Responses with window.ai</h4>
<p>To enable streaming AI responses, integrate <code>window.ai</code> within the nlux chat adapter.</p>
<pre class="astro-code monokai" style="background-color:#272822;color:#F8F8F2; overflow-x: auto;" tabindex="0"><code><span class="line"><span style="color:#F92672">import</span><span style="color:#F8F8F2"> { ChatAdapter, StreamingAdapterObserver } </span><span style="color:#F92672">from</span><span style="color:#E6DB74"> '@nlux/react'</span><span style="color:#F8F8F2">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672">export</span><span style="color:#66D9EF;font-style:italic"> const</span><span style="color:#F8F8F2"> streamAdapter</span><span style="color:#F92672">:</span><span> </span><span style="color:#A6E22E;text-decoration:underline">ChatAdapter</span><span style="color:#F92672"> =</span><span style="color:#F8F8F2"> {</span></span>
<span class="line"><span style="color:#A6E22E">    streamText</span><span style="color:#F8F8F2">: </span><span style="color:#F92672">async</span><span style="color:#F8F8F2"> (</span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">        prompt</span><span style="color:#F92672">:</span><span style="color:#66D9EF;font-style:italic"> string</span><span style="color:#F8F8F2">,</span></span>
<span class="line"><span style="color:#FD971F;font-style:italic">        observer</span><span style="color:#F92672">:</span><span> </span><span style="color:#A6E22E;text-decoration:underline">StreamingAdapterObserver</span><span style="color:#F8F8F2">,</span></span>
<span class="line"><span style="color:#F8F8F2">    ) </span><span style="color:#66D9EF;font-style:italic">=></span><span style="color:#F8F8F2"> {</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic">        const</span><span style="color:#F8F8F2"> canCreate </span><span style="color:#F92672">=</span><span style="color:#F92672"> await</span><span style="color:#F8F8F2"> window.ai.</span><span style="color:#A6E22E">canCreateTextSession</span><span style="color:#F8F8F2">();</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F92672">        if</span><span style="color:#F8F8F2"> (canCreate </span><span style="color:#F92672">!==</span><span style="color:#E6DB74"> "no"</span><span style="color:#F8F8F2">) {</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic">            const</span><span style="color:#F8F8F2"> session </span><span style="color:#F92672">=</span><span style="color:#F92672"> await</span><span style="color:#F8F8F2"> window.ai.</span><span style="color:#A6E22E">createTextSession</span><span style="color:#F8F8F2">();</span></span>
<span class="line"><span style="color:#66D9EF;font-style:italic">            const</span><span style="color:#F8F8F2"> stream </span><span style="color:#F92672">=</span><span style="color:#F8F8F2"> session.</span><span style="color:#A6E22E">promptStreaming</span><span style="color:#F8F8F2">(prompt);</span></span>
<span class="line"><span style="color:#F92672">            for</span><span style="color:#F92672"> await</span><span style="color:#F8F8F2"> (</span><span style="color:#66D9EF;font-style:italic">const</span><span style="color:#F8F8F2"> chunk </span><span style="color:#F92672">of</span><span style="color:#F8F8F2"> stream) {</span></span>
<span class="line"><span style="color:#F8F8F2">                observer.</span><span style="color:#A6E22E">next</span><span style="color:#F8F8F2">(chunk);</span></span>
<span class="line"><span style="color:#F8F8F2">            }</span></span>
<span class="line"><span style="color:#F8F8F2">        }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F8F8F2">        observer.</span><span style="color:#A6E22E">complete</span><span style="color:#F8F8F2">();</span></span>
<span class="line"><span style="color:#F8F8F2">    },</span></span>
<span class="line"><span style="color:#F8F8F2">};</span></span></code></pre>
<p>To use this code, you will need to install a version of chrome that has Google Gemini Nano enabled (at this time of writing you will need chrome dev). To preview this logic you can go to <a href="localai.space">https://localai.space/chat</a>.</p>
<h3 id="exploring-googles-built-in-ai">Exploring Google’s Built-In AI</h3>
<p>Google is developing web platform APIs and browser features to integrate AI models directly into browsers, including the Gemini Nano model. This approach aims to make AI tasks more accessible and efficient by leveraging local device capabilities.</p>
<h4 id="benefits-of-built-in-ai">Benefits of Built-In AI</h4>
<ol>
<li><strong>Ease of Deployment</strong>: The browser manages model updates and optimizations, reducing developer overhead.</li>
<li><strong>Hardware Acceleration</strong>: The AI runtime optimizes performance using available hardware, such as GPUs or NPUs.</li>
<li><strong>Local Processing</strong>: Sensitive data is processed locally, enhancing privacy.</li>
<li><strong>Snappy User Experience</strong>: On-device AI reduces latency, providing near-instant results.</li>
<li><strong>Offline AI Usage</strong>: AI functionalities remain available even without internet connectivity.</li>
</ol>
<h3 id="implementing-googles-built-in-ai">Implementing Google’s Built-In AI</h3>
<p>To utilize Google’s built-in AI features, developers can access task APIs for various AI-powered functionalities, such as summarization and translation. These APIs are designed to run inference against Gemini Nano, a highly efficient LLM optimized for local execution.</p>
<h4 id="early-preview-and-feedback">Early Preview and Feedback</h4>
<p>Google offers an early preview program to gather feedback and refine these APIs. Developers are encouraged to join this program to test in-progress features and contribute to the standardization process.</p>
<h3 id="conclusion">Conclusion</h3>
<p>The shift towards on-device AI represents a significant advancement in addressing privacy concerns and improving user experience. By leveraging libraries like nlux and upcoming browser-integrated AI features from Google, developers can build powerful, privacy-first AI applications that operate efficiently on local devices. Embracing these technologies ensures that AI continues to evolve in a manner that prioritizes user data security and accessibility.</p>
<h3 id="additional-resources">Additional Resources</h3>
<ul>
<li><a href="https://nlux.dev/docs">nlux Documentation</a></li>
<li><a href="https://developers.google.com/ai/javascript">Google AI JavaScript SDK</a></li>
<li><a href="https://groups.google.com/a/chromium.org/g/chrome-ai-developer-public">Join the Chrome AI Developer Public Announcements Group</a></li>
</ul>
<p>By integrating these tools and approaches, developers can harness the power of LLMs while maintaining a strong commitment to privacy and user experience.</p> </astro-slot></content></div></div></div></div><!--astro:end--></astro-island>  </div> <div class="mx-auto max-w-screen-lg px-3 py-6"><div class="no-print border-t border-gray-600 pt-5"><div class="text-sm text-gray-200">© Copyright <!-- -->2025<!-- --> by <!-- -->Dhanushka&#x27;s Blog<!-- -->. Built with ♥ by<!-- --> <a class="text-cyan-400 hover:underline" href="https://github.com/dhanushka2001" target="_blank" rel="noopener noreferrer">Dhanushka</a>. Last updated on <!-- -->2025-07-08<!-- -->.</div></div><script src="https://cdn.botpress.cloud/webchat/v0/inject.js"></script><script src="https://mediafiles.botpress.cloud/0df54034-3318-451a-aed0-3923a4ee25a5/webchat/config.js" defer=""></script></div> </body></html>